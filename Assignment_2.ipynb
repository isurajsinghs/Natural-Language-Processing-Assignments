{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assignment 2\n",
    "#Consider a English Paragraph apply Tokenization(Sentence Tokenizer,Word Tokenizer,WhiteSpaceTokenizer,TreebankWordTokenizer),Stemming,Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize,WhitespaceTokenizer,TreebankWordTokenizer\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stpwords = set(stopwords.words('english'))\n",
    "panctuations = list(string.punctuation) #Lists out all the Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ('The ambition of the greatest man of our generation has been to wipe every '\n",
    "            'tear from every eye. That may be beyond us, but so long as there are tears and ' \n",
    "            'suffering, so long our work will not be over. And so we have to labour and to ' \n",
    "            'work, and work hard, to give reality to our dreams. Those dreams are for India, ' \n",
    "            'but they are also for the world better.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The ambition of the greatest man of our generation has been to wipe every tear from every eye.',\n",
       " 'That may be beyond us, but so long as there are tears and suffering, so long our work will not be over.',\n",
       " 'And so we have to labour and to work, and work hard, to give reality to our dreams.',\n",
       " 'Those dreams are for India, but they are also for the world better.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Divide into the sentences\n",
    "sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'ambition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'greatest',\n",
       " 'man',\n",
       " 'of',\n",
       " 'our',\n",
       " 'generation',\n",
       " 'has',\n",
       " 'been',\n",
       " 'to',\n",
       " 'wipe',\n",
       " 'every',\n",
       " 'tear',\n",
       " 'from',\n",
       " 'every',\n",
       " 'eye',\n",
       " '.',\n",
       " 'That',\n",
       " 'may',\n",
       " 'be',\n",
       " 'beyond',\n",
       " 'us',\n",
       " ',',\n",
       " 'but',\n",
       " 'so',\n",
       " 'long',\n",
       " 'as',\n",
       " 'there',\n",
       " 'are',\n",
       " 'tears',\n",
       " 'and',\n",
       " 'suffering',\n",
       " ',',\n",
       " 'so',\n",
       " 'long',\n",
       " 'our',\n",
       " 'work',\n",
       " 'will',\n",
       " 'not',\n",
       " 'be',\n",
       " 'over',\n",
       " '.',\n",
       " 'And',\n",
       " 'so',\n",
       " 'we',\n",
       " 'have',\n",
       " 'to',\n",
       " 'labour',\n",
       " 'and',\n",
       " 'to',\n",
       " 'work',\n",
       " ',',\n",
       " 'and',\n",
       " 'work',\n",
       " 'hard',\n",
       " ',',\n",
       " 'to',\n",
       " 'give',\n",
       " 'reality',\n",
       " 'to',\n",
       " 'our',\n",
       " 'dreams',\n",
       " '.',\n",
       " 'Those',\n",
       " 'dreams',\n",
       " 'are',\n",
       " 'for',\n",
       " 'India',\n",
       " ',',\n",
       " 'but',\n",
       " 'they',\n",
       " 'are',\n",
       " 'also',\n",
       " 'for',\n",
       " 'the',\n",
       " 'world',\n",
       " 'better',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word Tokenization(Divide the sentences into words)\n",
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'ambition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'greatest',\n",
       " 'man',\n",
       " 'of',\n",
       " 'our',\n",
       " 'generation',\n",
       " 'has',\n",
       " 'been',\n",
       " 'to',\n",
       " 'wipe',\n",
       " 'every',\n",
       " 'tear',\n",
       " 'from',\n",
       " 'every',\n",
       " 'eye.',\n",
       " 'That',\n",
       " 'may',\n",
       " 'be',\n",
       " 'beyond',\n",
       " 'us,',\n",
       " 'but',\n",
       " 'so',\n",
       " 'long',\n",
       " 'as',\n",
       " 'there',\n",
       " 'are',\n",
       " 'tears',\n",
       " 'and',\n",
       " 'suffering,',\n",
       " 'so',\n",
       " 'long',\n",
       " 'our',\n",
       " 'work',\n",
       " 'will',\n",
       " 'not',\n",
       " 'be',\n",
       " 'over.',\n",
       " 'And',\n",
       " 'so',\n",
       " 'we',\n",
       " 'have',\n",
       " 'to',\n",
       " 'labour',\n",
       " 'and',\n",
       " 'to',\n",
       " 'work,',\n",
       " 'and',\n",
       " 'work',\n",
       " 'hard,',\n",
       " 'to',\n",
       " 'give',\n",
       " 'reality',\n",
       " 'to',\n",
       " 'our',\n",
       " 'dreams.',\n",
       " 'Those',\n",
       " 'dreams',\n",
       " 'are',\n",
       " 'for',\n",
       " 'India,',\n",
       " 'but',\n",
       " 'they',\n",
       " 'are',\n",
       " 'also',\n",
       " 'for',\n",
       " 'the',\n",
       " 'world',\n",
       " 'better.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#White Space Tokenization(Tokenize the sentence based on space)\n",
    "WhitespaceTokenizer().tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'ambition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'greatest',\n",
       " 'man',\n",
       " 'of',\n",
       " 'our',\n",
       " 'generation',\n",
       " 'has',\n",
       " 'been',\n",
       " 'to',\n",
       " 'wipe',\n",
       " 'every',\n",
       " 'tear',\n",
       " 'from',\n",
       " 'every',\n",
       " 'eye.',\n",
       " 'That',\n",
       " 'may',\n",
       " 'be',\n",
       " 'beyond',\n",
       " 'us',\n",
       " ',',\n",
       " 'but',\n",
       " 'so',\n",
       " 'long',\n",
       " 'as',\n",
       " 'there',\n",
       " 'are',\n",
       " 'tears',\n",
       " 'and',\n",
       " 'suffering',\n",
       " ',',\n",
       " 'so',\n",
       " 'long',\n",
       " 'our',\n",
       " 'work',\n",
       " 'will',\n",
       " 'not',\n",
       " 'be',\n",
       " 'over.',\n",
       " 'And',\n",
       " 'so',\n",
       " 'we',\n",
       " 'have',\n",
       " 'to',\n",
       " 'labour',\n",
       " 'and',\n",
       " 'to',\n",
       " 'work',\n",
       " ',',\n",
       " 'and',\n",
       " 'work',\n",
       " 'hard',\n",
       " ',',\n",
       " 'to',\n",
       " 'give',\n",
       " 'reality',\n",
       " 'to',\n",
       " 'our',\n",
       " 'dreams.',\n",
       " 'Those',\n",
       " 'dreams',\n",
       " 'are',\n",
       " 'for',\n",
       " 'India',\n",
       " ',',\n",
       " 'but',\n",
       " 'they',\n",
       " 'are',\n",
       " 'also',\n",
       " 'for',\n",
       " 'the',\n",
       " 'world',\n",
       " 'better',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uses regular expressions to tokenize text\n",
    "TreebankWordTokenizer().tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Root Word           \n",
      "The                 the                 \n",
      "ambition            ambit               \n",
      "greatest            greatest            \n",
      "man                 man                 \n",
      "generation          gener               \n",
      "wipe                wipe                \n",
      "every               everi               \n",
      "tear                tear                \n",
      "every               everi               \n",
      "eye                 eye                 \n",
      "That                that                \n",
      "may                 may                 \n",
      "beyond              beyond              \n",
      "us                  us                  \n",
      "long                long                \n",
      "tears               tear                \n",
      "suffering           suffer              \n",
      "long                long                \n",
      "work                work                \n",
      "And                 and                 \n",
      "labour              labour              \n",
      "work                work                \n",
      "work                work                \n",
      "hard                hard                \n",
      "give                give                \n",
      "reality             realiti             \n",
      "dreams              dream               \n",
      "Those               those               \n",
      "dreams              dream               \n",
      "India               india               \n",
      "also                also                \n",
      "world               world               \n",
      "better              better              \n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "sentences = sent_tokenize(sentence)\n",
    "stemmer = PorterStemmer()\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Root Word\"))\n",
    "for i in range(len(sentences)):\n",
    "    words = word_tokenize(sentences[i])\n",
    "    for word in words:\n",
    "        if (word not in stpwords and word not in panctuations):\n",
    "            print(\"{0:20}{1:20}\".format(word,stemmer.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "The                 The                 \n",
      "ambition            ambition            \n",
      "greatest            great               \n",
      "man                 man                 \n",
      "generation          generation          \n",
      "wipe                wipe                \n",
      "every               every               \n",
      "tear                tear                \n",
      "every               every               \n",
      "eye                 eye                 \n",
      "That                That                \n",
      "may                 may                 \n",
      "beyond              beyond              \n",
      "us                  u                   \n",
      "long                long                \n",
      "tears               tear                \n",
      "suffering           suffering           \n",
      "long                long                \n",
      "work                work                \n",
      "And                 And                 \n",
      "labour              labour              \n",
      "work                work                \n",
      "work                work                \n",
      "hard                hard                \n",
      "give                give                \n",
      "reality             reality             \n",
      "dreams              dream               \n",
      "Those               Those               \n",
      "dreams              dream               \n",
      "India               India               \n",
      "also                also                \n",
      "world               world               \n",
      "better              well                \n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "#Function to convert nltk tag to wordnet tag\n",
    "def nltk_to_wordnet(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "sentences = sent_tokenize(sentence)\n",
    "lematizer = WordNetLemmatizer()\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for i in range(len(sentences)):\n",
    "    words = word_tokenize(sentences[i])\n",
    "    nltk_tagged = pos_tag(words)\n",
    "    wordnet_tagged = map(lambda x:(x[0],nltk_to_wordnet(x[1])),nltk_tagged)\n",
    "    for word,tag in wordnet_tagged:\n",
    "        if (word not in stpwords and word not in panctuations):\n",
    "            if tag is not None:\n",
    "                print(\"{0:20}{1:20}\".format(word,lematizer.lemmatize(word,tag)))\n",
    "            else:\n",
    "                print(\"{0:20}{1:20}\".format(word,lematizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I----->None\n",
      "am----->v\n",
      "going----->v\n",
      "to----->None\n",
      "school----->n\n"
     ]
    }
   ],
   "source": [
    "nltk_taged = pos_tag(['I','am','going','to','school'])\n",
    "wordnet_tagged = map(lambda x:(x[0],nltk_to_wordnet(x[1])),nltk_taged)\n",
    "for word,tag in wordnet_tagged:\n",
    "    print(f\"{word}----->{tag}\")\n",
    "                     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
